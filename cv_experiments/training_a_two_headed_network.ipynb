{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a two headed model (using a pretrained backbone). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "random.seed(4)\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_celeba_attributes():\n",
    "    return ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips',\n",
    "            'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby',\n",
    "            'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male',\n",
    "            'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
    "            'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
    "            'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
    "            'Wearing_Necktie', 'Young']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.mobilenet_v3_small(weights='MobileNet_V3_Small_Weights.IMAGENET1K_V1') #this can be changed to resnet50 etc\n",
    "model.classifier[3]=torch.nn.Linear(1024,2) # Modify the model to have two heads i.e. predict two classes. One head is for the target attribute (in your case this would be for hate speech detected). \n",
    "#The other head we put a squared loss on (this is for the protected attribute --- in the text data this would be gender/race etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for the dataset\n",
    "data_root = './data/'\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 224\n",
    "\n",
    "\n",
    "celeba_train = datasets.CelebA(data_root,split='train',\n",
    "                              download=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Resize(256),\n",
    "                                  transforms.RandomCrop(224),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                       std=[0.229, 0.224, 0.225])\n",
    "                              ]))\n",
    "celeba_val = datasets.CelebA(data_root, split='valid',\n",
    "                              download=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Resize(256),\n",
    "                                  transforms.CenterCrop(224),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                       std=[0.229, 0.224, 0.225])\n",
    "                              ]))\n",
    "\n",
    "celeba_test = datasets.CelebA(data_root, split='test',\n",
    "                              download=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Resize(256),\n",
    "                                  transforms.CenterCrop(224),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                       std=[0.229, 0.224, 0.225])\n",
    "                              ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_celeba_attributes().index(\"Smiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loss = torch.nn.BCEWithLogitsLoss()\n",
    "attribute_loss = torch.nn.MSELoss()\n",
    "target_class=9\n",
    "protected_class=20\n",
    "\n",
    "\n",
    "scaling_factor=0.5 # factor of 0.5 used by lohaus (this is the weight on the second head --- protected attribute)\n",
    " \n",
    "def total_loss(y,pred):\n",
    "    \"Two headed training loss\"\n",
    "    if y.dim() == 1:\n",
    "        y = y.unsqueeze(0)\n",
    "    y = y.type(torch.float32)    \n",
    "    tl = target_loss(pred[:, 0],y[:, target_class])\n",
    "    al = attribute_loss(pred[:,1],y[:, protected_class])\n",
    "    return tl + al*scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "\n",
    "\n",
    "class LitTwoHead(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def my_logging(self,loss,y,pred):\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log('head 1 loss',target_loss(pred[:, 0],y[:, target_class].type(torch.float32)))\n",
    "        self.log('head 2 loss',attribute_loss(pred[:, 1],y[:, protected_class].type(torch.float32)))\n",
    "        self.log('accuracy (head 1)',((pred[:,0]<=0)==(y[:,target_class]<=0)).type(torch.float32).mean())\n",
    "        self.log('accuracy (head 2)',((pred[:,1]<=0.5)==(y[:,protected_class]<=0)).type(torch.float32).mean())\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss = total_loss(y, pred)\n",
    "        self.my_logging(loss, y, pred)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "        return optimizer\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss = total_loss(y, pred)\n",
    "        self.my_logging(loss, y, pred)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #Same statistics as test\n",
    "        self.test_step(batch, batch_idx)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer=LitTwoHead(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(celeba_train, batch_size = 32, num_workers=9,persistent_workers=True)\n",
    "val_loader = utils.data.DataLoader(celeba_val, batch_size = 32, num_workers=9,persistent_workers=True)\n",
    "test_loader = utils.data.DataLoader(celeba_test, batch_size = 32,num_workers=9,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX 3500 Ada Generation Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | MobileNetV3 | 1.5 M \n",
      "--------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.080     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d99824fc7048cc94d60ea39b4892be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(limit_train_batches=500,limit_val_batches=False, max_epochs=20) #I've changed these numbers to make training much faster   \n",
    "trainer.fit(model=model_trainer, train_dataloaders=train_loader ,val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cfeed409ca4f018afa6cc7541325a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    accuracy (head 1)       0.9501051902770996\n",
      "    accuracy (head 2)       0.9630297422409058\n",
      "       head 1 loss           0.292962908744812\n",
      "       head 2 loss          0.03179924562573433\n",
      "       train_loss           0.3088625967502594\n",
      "─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': 0.3088625967502594,\n",
       "  'head 1 loss': 0.292962908744812,\n",
       "  'head 2 loss': 0.03179924562573433,\n",
       "  'accuracy (head 1)': 0.9501051902770996,\n",
       "  'accuracy (head 2)': 0.9630297422409058}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "trainer.test(model_trainer, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model outputs and the true and predicted attributes for using in OxonFair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "import numpy as np\n",
    "outputs_val =np.zeros((len(celeba_val),2))\n",
    "for i,data in enumerate(celeba_val):\n",
    "     outputs_val[i]=model(data[0].unsqueeze(0)).detach()\n",
    "np.save('prototyping_outputs_val.npy', outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test =np.zeros((len(celeba_test),2))\n",
    "for i,data in enumerate(celeba_test):\n",
    "     outputs_test[i]=model(data[0].unsqueeze(0)).detach()\n",
    "np.save('prototyping_outputs_test.npy', outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_val.attr[:, target_class].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_val.attr[:,20].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_test.attr[:, target_class].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_test.attr[:, 20].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protected_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('prototyping_target_label_val.npy', celeba_val.attr[:, target_class].numpy())\n",
    "np.save('prototyping_protected_label_val.npy', celeba_val.attr[:, protected_class].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('prototyping_target_label_test.npy', celeba_test.attr[:, target_class].numpy())\n",
    "np.save('prototyping_protected_label_test.npy', celeba_test.attr[:, protected_class].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_test.shape[0] == celeba_test.attr[:, target_class].numpy().shape[0] == celeba_test.attr[:, target_class].numpy().shape[0] #checking same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_val.shape[0] == celeba_val.attr[:, target_class].numpy().shape[0] == celeba_val.attr[:, target_class].numpy().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162770"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(celeba_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
